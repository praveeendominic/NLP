{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author: Praveen Dominic\n",
    "!conda activate base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"SMSSpamCollection\", sep = '\\t', names=['label','text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label = pd.get_dummies(df.label).iloc[:,-1].values\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
       " 'Ok lar... Joking wif u oni...',\n",
       " \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = df.text.tolist()\n",
    "sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go jurong point crazy available bugis n great world la e buffet cine got amore wat',\n",
       " 'ok lar joking wif u oni',\n",
       " 'free entry wkly comp win fa cup final tkts st may text fa receive entry question std txt rate c apply']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    sentences[i] = re.sub(\"[^a-zA-Z]\",\" \", sentences[i]).lower()\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [WordNetLemmatizer().lemmatize(word) for word in words if word not in stopwords.words(\"english\")]\n",
    "    sentences[i] = \" \".join(words)\n",
    "sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   6, 3568,  279, ...,    0,    0,    0],\n",
       "       [   8,  246, 1255, ...,    0,    0,    0],\n",
       "       [  10,  351,  628, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [7115, 1100, 3565, ...,    0,    0,    0],\n",
       "       [ 134, 7116, 3523, ...,    0,    0,    0],\n",
       "       [2201,  332,  156, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(sentences)\n",
    "seq = tok.texts_to_sequences(sentences)\n",
    "\n",
    "maxlen = 250\n",
    "vocab_size = len(tok.word_index)+1\n",
    "\n",
    "padded_seq = pad_sequences(seq, padding='post', maxlen=maxlen)\n",
    "padded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"glove.twitter.27B.200d.txt\",encoding = 'utf-8')\n",
    "glove_dict = {}\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:])\n",
    "    glove_dict[word] = vector\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0.14965', '-0.31038', '0.10748', '-0.34144', '0.34958',\n",
       "       '-0.029698', '0.96534', '-0.52384', '-0.28422', '-0.19859',\n",
       "       '0.34893', '0.13223', '0.23002', '-0.13964', '-0.0015468',\n",
       "       '-0.11598', '0.22373', '0.12963', '0.14723', '0.16222', '0.094383',\n",
       "       '0.14253', '-0.42843', '-0.85952', '-0.15597', '0.89196',\n",
       "       '0.081472', '-0.93562', '-0.29099', '-0.12727', '-0.13129',\n",
       "       '0.25275', '-0.66842', '-0.17604', '0.12941', '-0.17072',\n",
       "       '-0.2801', '0.017041', '0.1665', '0.20012', '-0.58155', '0.28237',\n",
       "       '0.23157', '0.44144', '-0.67181', '0.29037', '-0.22129', '0.22202',\n",
       "       '-0.092889', '0.31896', '0.071365', '-0.056723', '-0.50401',\n",
       "       '-0.079147', '0.14696', '-0.27105', '-0.29713', '-0.31681',\n",
       "       '0.30087', '-0.22464', '0.54422', '-0.3454', '0.093015', '0.20995',\n",
       "       '0.3525', '0.21563', '0.2591', '0.86872', '0.017481', '0.52115',\n",
       "       '-0.76771', '0.020134', '-0.21144', '0.14976', '0.33712',\n",
       "       '0.68312', '-0.17069', '-0.042784', '-0.0088607', '-0.46106',\n",
       "       '0.8521', '0.02202', '0.019392', '-0.037735', '0.38964',\n",
       "       '-0.025908', '-0.16752', '-0.14153', '0.18397', '0.065303',\n",
       "       '-0.31272', '0.19572', '-0.0016572', '0.19884', '0.5141',\n",
       "       '-0.069085', '-0.55525', '-0.1419', '0.63713', '0.44912',\n",
       "       '-0.55873', '-0.91263', '0.057638', '-0.26789', '-0.20454',\n",
       "       '-0.19631', '-0.36836', '0.02912', '0.12359', '-0.0913', '0.15062',\n",
       "       '-0.69245', '-0.64932', '-0.40185', '0.070017', '0.23845',\n",
       "       '-0.1413', '-0.31885', '-0.37702', '-0.53164', '-0.54867',\n",
       "       '0.10876', '-0.12807', '-0.22548', '0.1279', '0.28262', '0.30624',\n",
       "       '0.059435', '0.36049', '-0.25613', '-0.32646', '0.0085166',\n",
       "       '-0.25065', '-0.013681', '-0.2467', '1.0263', '0.48572', '0.35299',\n",
       "       '-0.29188', '0.29779', '-0.35197', '0.62835', '0.0096422',\n",
       "       '0.072962', '-0.47038', '-0.248', '0.37894', '0.17306',\n",
       "       '-0.059475', '-0.13325', '0.1224', '0.47439', '-4.0092', '0.05429',\n",
       "       '-0.32139', '-0.1685', '0.1998', '-0.054798', '-0.0024567',\n",
       "       '0.066554', '-0.12002', '-0.14643', '-0.18613', '-0.025009',\n",
       "       '0.21232', '0.059417', '-0.37315', '0.57723', '-0.12798',\n",
       "       '0.34772', '-0.085054', '0.27925', '-0.089443', '0.062465',\n",
       "       '-0.38764', '0.77186', '0.25611', '0.43627', '-0.36112',\n",
       "       '-0.37211', '-0.33438', '-0.57802', '-0.10035', '0.2407',\n",
       "       '-0.28136', '0.42337', '-0.69345', '0.83006', '0.43859',\n",
       "       '-0.25583', '0.12416', '0.54705', '-0.18504', '-0.15725',\n",
       "       '0.12429', '0.57767', '-0.3742', '0.079809', '-0.70911',\n",
       "       '-0.32481'], dtype='<U10')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_dict.get(\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_dict.get(\"awsome\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pobox\n",
      "optout\n",
      "chikku\n",
      "mobileupd\n",
      "freemsg\n",
      "savamob\n",
      "bslvyl\n",
      "callertune\n",
      "custcare\n",
      "rakhesh\n",
      "getzed\n",
      "ldew\n",
      "comuk\n",
      "ppmx\n",
      "fullonsms\n",
      "yijue\n",
      "shuhui\n",
      "tscs\n",
      "stoptxt\n",
      "aathi\n",
      "txtauction\n",
      "sipix\n",
      "urawinner\n",
      "toclaim\n",
      "nimya\n",
      "ldnw\n",
      "mtmsg\n",
      "themob\n",
      "geeee\n",
      "prabha\n",
      "yogasana\n",
      "videophones\n",
      "noline\n",
      "rentl\n",
      "linerental\n",
      "unredeemed\n",
      "ansr\n",
      "netcollex\n",
      "bcums\n",
      "kettoda\n",
      "belovd\n",
      "kusruthi\n",
      "mtmsgrcvd\n",
      "skilgme\n",
      "winawk\n",
      "qxj\n",
      "jamster\n",
      "msgrcvdhg\n",
      "textpod\n",
      "freefone\n",
      "thangam\n",
      "instituitions\n",
      "minnaminunginte\n",
      "nurungu\n",
      "vettam\n",
      "xuhui\n",
      "suprman\n",
      "maneesha\n",
      "ranjith\n",
      "lvblefrnd\n",
      "jstfrnd\n",
      "cutefrnd\n",
      "lifpartnr\n",
      "swtheart\n",
      "mobilesdirect\n",
      "arng\n",
      "iouri\n",
      "fightng\n",
      "soryda\n",
      "ibhltd\n",
      "ummmmmaah\n",
      "tirupur\n",
      "textoperator\n",
      "csbcm\n",
      "callcost\n",
      "perwksub\n",
      "subpoly\n",
      "shesil\n",
      "polyph\n",
      "wkg\n",
      "pocketbabe\n",
      "textcomp\n",
      "grahmbell\n",
      "invnted\n",
      "minmobsmorelkpobox\n",
      "ringtoneking\n",
      "skillgame\n",
      "winaweek\n",
      "ppermesssubscription\n",
      "shijas\n",
      "deliveredtomorrow\n",
      "dubsack\n",
      "senthil\n",
      "receivea\n",
      "tsandcs\n",
      "dbuk\n",
      "lccltd\n",
      "xxxmobilemovieclub\n",
      "eurodisinc\n",
      "morefrmmob\n",
      "shracomorsglsuplt\n",
      "bootydelious\n",
      "keralacircle\n",
      "deeraj\n",
      "prepayment\n",
      "tissco\n",
      "tayseer\n",
      "inshah\n",
      "loxahatchee\n",
      "txttowin\n",
      "mobno\n",
      "txtno\n",
      "geeeee\n",
      "textbuddy\n",
      "gaytextbuddy\n",
      "expressoffer\n",
      "rstm\n",
      "mobilesvary\n",
      "ctxt\n",
      "cashto\n",
      "getstop\n",
      "mojibiola\n",
      "lovejen\n",
      "smsco\n",
      "fixedline\n",
      "stopsms\n",
      "datebox\n",
      "essexcm\n",
      "wkent\n",
      "speedchat\n",
      "itcould\n",
      "minmoremobsemspobox\n",
      "nydc\n",
      "lionm\n",
      "lionp\n",
      "tirunelvali\n",
      "icicibank\n",
      "kaiez\n",
      "lyfu\n",
      "msgrcvd\n",
      "musthu\n",
      "jsco\n",
      "exmpel\n",
      "splleing\n",
      "wrnog\n",
      "dippeditinadew\n",
      "itwhichturnedinto\n",
      "tomeandsaid\n",
      "addamsfa\n",
      "movietrivia\n",
      "maangalyam\n",
      "alaipayuthe\n",
      "callfreefone\n",
      "hallaq\n",
      "idew\n",
      "hrishi\n",
      "gamestar\n",
      "propsd\n",
      "lttrs\n",
      "evrydy\n",
      "yarasu\n",
      "vaazhthukkal\n",
      "indyarocks\n",
      "yunny\n",
      "picsfree\n",
      "helloooo\n",
      "visionsms\n",
      "agalla\n",
      "otside\n",
      "spile\n",
      "jiayin\n",
      "waxsto\n",
      "cdgt\n",
      "shahjahan\n",
      "kavalan\n",
      "langport\n",
      "confirmd\n",
      "hppnss\n",
      "mtalk\n",
      "widelive\n",
      "kothi\n",
      "natalja\n",
      "polyh\n",
      "txtstop\n",
      "cashbin\n",
      "nuther\n",
      "goldviking\n",
      "notxt\n",
      "chgs\n",
      "qjkgighjjgcbl\n",
      "poboxox\n",
      "ffffffffff\n",
      "accomodations\n",
      "lucyxx\n",
      "nevering\n",
      "aaooooright\n",
      "annoncement\n",
      "bangbabes\n",
      "bangb\n",
      "missunderstding\n",
      "areyouunique\n",
      "babyjontet\n",
      "ubandu\n",
      "flyng\n",
      "elama\n",
      "mudyadhu\n",
      "gandhipuram\n",
      "thirtyeight\n",
      "pleassssssseeeeee\n",
      "sportsx\n",
      "nipost\n",
      "sometme\n",
      "varunnathu\n",
      "edukkukayee\n",
      "womdarfull\n",
      "rodds\n",
      "icmb\n",
      "cktz\n",
      "winnersclub\n",
      "daaaaa\n",
      "huiming\n",
      "sextextuk\n",
      "xxuk\n",
      "regalportfolio\n",
      "payasam\n",
      "wahleykkum\n",
      "busetop\n",
      "werethe\n",
      "monkeespeople\n",
      "monkeyaround\n",
      "howu\n",
      "foundurself\n",
      "jobyet\n",
      "lotsly\n",
      "upgrdcentre\n",
      "okmail\n",
      "jaykwon\n",
      "falconerf\n",
      "nuerologist\n",
      "lolnice\n",
      "boltblue\n",
      "bubbletext\n",
      "tgxxrz\n",
      "applausestore\n",
      "monthlysubscription\n",
      "sugardad\n",
      "ninish\n",
      "arngd\n",
      "unfortuntly\n",
      "textand\n",
      "meive\n",
      "gotany\n",
      "clubsaisai\n",
      "kalstiya\n",
      "sdryb\n",
      "lapdancer\n",
      "ppmsg\n",
      "cherthala\n",
      "cnupdates\n",
      "wondar\n",
      "alertfrom\n",
      "stewartsize\n",
      "kbsubject\n",
      "prescripiton\n",
      "drvgsto\n",
      "caveboy\n",
      "closingdate\n",
      "claimcode\n",
      "pmmorefrommobile\n",
      "bremoved\n",
      "mobypobox\n",
      "weaseling\n",
      "beerage\n",
      "randomlly\n",
      "toppoly\n",
      "dogbreath\n",
      "whereare\n",
      "friendsare\n",
      "thekingshead\n",
      "canlove\n",
      "boooo\n",
      "yowifes\n",
      "simonwatson\n",
      "shinco\n",
      "smsrewards\n",
      "yourjob\n",
      "llspeak\n",
      "soonlots\n",
      "smsservices\n",
      "yourinclusive\n",
      "rvx\n",
      "prashanthettan\n",
      "urfeeling\n",
      "bettersn\n",
      "probthat\n",
      "sathy\n",
      "champneys\n",
      "mymoby\n",
      "getsleep\n",
      "studdying\n",
      "yoyyooo\n",
      "faglord\n",
      "ctter\n",
      "cttergg\n",
      "cttargg\n",
      "ctargg\n",
      "ctagg\n",
      "racal\n",
      "nookii\n",
      "grinule\n",
      "avalarr\n",
      "hollalater\n",
      "magicalsongs\n",
      "vomitin\n",
      "sophas\n",
      "ogunrinde\n",
      "eightish\n",
      "bluetoothhdset\n",
      "doublemins\n",
      "doubletxt\n",
      "poyyarikatur\n",
      "kolathupalayam\n",
      "unjalur\n",
      "ctla\n",
      "ishtamayoo\n",
      "bakrid\n",
      "souveniers\n",
      "lekdog\n",
      "flirtparty\n",
      "kicchu\n",
      "kaaj\n",
      "iccha\n",
      "korche\n",
      "pokkiri\n",
      "makiing\n",
      "ndship\n",
      "hourish\n",
      "cheyyamo\n",
      "stchoice\n",
      "computerless\n",
      "velachery\n",
      "cstore\n",
      "himso\n",
      "prakesh\n",
      "drivby\n",
      "edrunk\n",
      "pthis\n",
      "senrd\n",
      "dancce\n",
      "basq\n",
      "nhite\n",
      "westonzoyland\n",
      "melnite\n",
      "ifink\n",
      "downstem\n",
      "inperialmusic\n",
      "leafcutter\n",
      "didntgive\n",
      "bellearlier\n",
      "bedbut\n",
      "thepub\n",
      "uwana\n",
      "jenxxx\n",
      "wheellock\n",
      "janx\n",
      "videosound\n",
      "videosounds\n",
      "coveragd\n",
      "vasai\n",
      "youuuuu\n",
      "witot\n",
      "undrstndng\n",
      "everyso\n",
      "panicks\n",
      "thirunelvali\n",
      "noncomittal\n",
      "interviw\n",
      "wisheds\n",
      "rummer\n",
      "yalru\n",
      "astne\n",
      "mundhe\n",
      "edhae\n",
      "vargu\n",
      "meetins\n",
      "incomm\n",
      "kalainar\n",
      "thenampet\n",
      "aaniye\n",
      "pudunga\n",
      "venaam\n",
      "hhahhaahahah\n",
      "rawring\n",
      "worzels\n",
      "lnly\n",
      "rgent\n",
      "janinexx\n",
      "spageddies\n",
      "mobsi\n",
      "sonathaya\n",
      "soladha\n",
      "accidant\n",
      "tookplace\n",
      "ghodbandar\n",
      "slovely\n",
      "desparately\n",
      "trackmarque\n",
      "vipclub\n",
      "praps\n",
      "downon\n",
      "theacusations\n",
      "wotu\n",
      "haventcn\n",
      "conacted\n",
      "panren\n",
      "trainners\n",
      "dileep\n",
      "muchand\n",
      "venugopal\n",
      "remembrs\n",
      "urmom\n",
      "careabout\n",
      "comingdown\n",
      "engalnd\n",
      "jontin\n",
      "fffff\n",
      "vpod\n",
      "anyplaces\n",
      "minapn\n",
      "hittng\n",
      "shhhhh\n",
      "corrct\n",
      "havbeen\n",
      "havebeen\n",
      "preschoolco\n",
      "ordinator\n",
      "inlude\n",
      "ambrith\n",
      "marrge\n",
      "satsgettin\n",
      "subtoitles\n",
      "thkin\n",
      "resubbing\n",
      "wondarfull\n",
      "txtx\n",
      "onbus\n",
      "donyt\n",
      "latelyxxx\n",
      "justthought\n",
      "sayhey\n",
      "offdam\n",
      "tming\n",
      "samachara\n",
      "washob\n",
      "nobbing\n",
      "ponnungale\n",
      "ipaditan\n",
      "marsms\n",
      "utele\n",
      "qlynnbv\n",
      "stapati\n",
      "specialisation\n",
      "crickiting\n",
      "dontmatter\n",
      "urgoin\n",
      "reallyneed\n",
      "docd\n",
      "dontplease\n",
      "dontignore\n",
      "mycalls\n",
      "thecd\n",
      "yavnt\n",
      "fishhead\n",
      "beehoon\n",
      "jaklin\n",
      "avble\n",
      "aluable\n",
      "ffectionate\n",
      "oveable\n",
      "ternal\n",
      "ruthful\n",
      "ntimate\n",
      "atural\n",
      "namous\n",
      "waliking\n",
      "smidgin\n",
      "braindance\n",
      "ofstuff\n",
      "unmits\n",
      "yummmm\n",
      "puzzeles\n",
      "shifad\n",
      "tolerat\n",
      "splashmobile\n",
      "subscrition\n",
      "brdget\n",
      "stoptx\n",
      "tddnewsletter\n",
      "thedailydraw\n",
      "prizeswith\n",
      "jeetey\n",
      "varaya\n",
      "elaya\n",
      "workand\n",
      "whilltake\n",
      "zogtorius\n",
      "monoc\n",
      "polyc\n",
      "goodmate\n",
      "franyxxxxx\n",
      "becausethey\n",
      "neshanth\n",
      "byatch\n",
      "filthyguys\n",
      "reltnship\n",
      "premarica\n",
      "anjola\n",
      "asjesus\n",
      "taxless\n",
      "scorable\n",
      "suganya\n",
      "famamus\n",
      "sppok\n",
      "reppurcussions\n",
      "shoranur\n",
      "quizclub\n",
      "korli\n",
      "dabbles\n",
      "arestaurant\n",
      "dabooks\n",
      "tkls\n",
      "stoptxtstop\n",
      "dramastorm\n",
      "sundayish\n",
      "olowoyey\n",
      "taxt\n",
      "scarcasim\n",
      "eruku\n",
      "modl\n",
      "witin\n",
      "nnfwfly\n",
      "justbeen\n",
      "kanagu\n",
      "bcmsfwc\n",
      "mittelschmertz\n",
      "cthen\n",
      "crazyin\n",
      "sleepingwith\n",
      "weiyi\n",
      "romcapspam\n",
      "sweatter\n",
      "miiiiiiissssssssss\n",
      "gudni\n",
      "stopbcm\n",
      "groovying\n",
      "transfred\n",
      "doinat\n",
      "callon\n",
      "pdate\n",
      "yhl\n",
      "woozles\n",
      "buzzzz\n",
      "doesdiscount\n",
      "shitinnit\n",
      "hogidhe\n",
      "chinnu\n",
      "swalpa\n",
      "agidhane\n",
      "footbl\n",
      "crckt\n",
      "pocy\n",
      "onluy\n",
      "offcampus\n",
      "chrgd\n",
      "dasara\n",
      "rememberi\n",
      "somewheresomeone\n",
      "tosend\n",
      "frndsship\n",
      "slaaaaave\n",
      "gotbabes\n",
      "gopalettan\n",
      "xxsp\n",
      "stopcost\n",
      "wnevr\n",
      "madodu\n",
      "pretsorginta\n",
      "nammanna\n",
      "pretsovru\n",
      "stdtxtrate\n",
      "phyhcmk\n",
      "stagwood\n",
      "winterstone\n",
      "pathaya\n",
      "enketa\n",
      "maraikara\n",
      "punj\n",
      "mmsto\n",
      "dungerees\n",
      "mentionned\n",
      "hogolo\n",
      "kodstini\n",
      "madstini\n",
      "hogli\n",
      "eerulli\n",
      "kodthini\n",
      "thasa\n",
      "shudvetold\n",
      "urgran\n",
      "illspeak\n",
      "mecause\n",
      "werebored\n",
      "okden\n",
      "likeyour\n",
      "countinlots\n",
      "coccooning\n",
      "tirunelvai\n",
      "phews\n",
      "squeeeeeze\n",
      "raviyog\n",
      "bhayandar\n",
      "badrith\n",
      "vatian\n",
      "gonnamissu\n",
      "buttheres\n",
      "aboutas\n",
      "merememberin\n",
      "asthere\n",
      "ofsi\n",
      "yaxx\n",
      "poortiyagi\n",
      "odalebeku\n",
      "hanumanji\n",
      "bajarangabali\n",
      "pavanaputra\n",
      "sankatmochan\n",
      "ramaduth\n",
      "mahaveer\n",
      "janarige\n",
      "ivatte\n",
      "kalisidare\n",
      "olage\n",
      "ondu\n",
      "keluviri\n",
      "maretare\n",
      "dodda\n",
      "siguviri\n",
      "neglet\n",
      "workage\n",
      "checkboxes\n",
      "yifeng\n",
      "sariyag\n",
      "madoke\n",
      "barolla\n",
      "nigpun\n",
      "dismissial\n",
      "wellda\n",
      "studentfinancial\n",
      "uhhhhrmm\n",
      "deltomorrow\n",
      "smartcall\n",
      "subscriptn\n",
      "landlineonly\n",
      "minmobsmore\n",
      "lkpobox\n",
      "youphone\n",
      "youwanna\n",
      "hypotheticalhuagauahahuagahyuhagga\n",
      "cantdo\n",
      "anythingtomorrow\n",
      "aretaking\n",
      "outfor\n",
      "katexxx\n",
      "petticoatdreams\n",
      "weddingfriend\n",
      "reslove\n",
      "notixiquating\n",
      "laxinorficated\n",
      "bambling\n",
      "entropication\n",
      "oblisingately\n",
      "masteriastering\n",
      "amplikater\n",
      "fidalfication\n",
      "champlaxigating\n",
      "nikiyu\n",
      "xxxxxxxxxxxxxx\n",
      "eppolum\n",
      "allalo\n",
      "shldxxxx\n",
      "nytho\n",
      "fonin\n",
      "frmcloud\n",
      "ppmpobox\n",
      "bhamb\n",
      "saristar\n",
      "mobcudb\n",
      "sabarish\n",
      "inpersonation\n",
      "banneduk\n",
      "sphosting\n",
      "webadres\n",
      "ltdhelpdesk\n",
      "entey\n",
      "nattil\n",
      "kittum\n",
      "interfued\n",
      "underdtand\n",
      "muchxxlove\n",
      "locaxx\n",
      "apnt\n",
      "wherevr\n",
      "ything\n",
      "lubly\n",
      "nusstu\n",
      "hudgi\n",
      "yorge\n",
      "pataistha\n",
      "ertini\n",
      "ummifying\n",
      "txtstar\n",
      "aldrine\n",
      "necesity\n",
      "shangela\n",
      "gloucesterroad\n",
      "fishrman\n",
      "mobstorequiz\n",
      "praveesh\n",
      "scallies\n",
      "subletting\n",
      "ammae\n",
      "lambu\n",
      "batchlor\n",
      "cncl\n",
      "stopcs\n",
      "ecef\n",
      "xafter\n",
      "yaxxx\n",
      "pushbutton\n",
      "babygoodbye\n",
      "webeburnin\n",
      "guoyang\n",
      "dengra\n",
      "toplay\n",
      "fieldof\n",
      "selfindependence\n",
      "gnarl\n",
      "fakeye\n",
      "eckankar\n",
      "asssssholeeee\n",
      "dizzamn\n",
      "nimbomsons\n",
      "thousad\n",
      "onlyfound\n",
      "cusoon\n",
      "macleran\n",
      "trishul\n",
      "yalrigu\n",
      "heltini\n",
      "esaplanade\n",
      "kalaachutaarama\n",
      "ldns\n",
      "persolvo\n",
      "noworriesloans\n",
      "nbme\n",
      "muhommad\n",
      "gsoh\n",
      "mjzgroup\n",
      "hlday\n",
      "amrca\n",
      "panalam\n",
      "spjanuary\n",
      "wtlp\n",
      "swhrt\n",
      "jetton\n",
      "prizeawaiting\n",
      "tuth\n",
      "closeby\n",
      "bookedthe\n",
      "eshxxxxxxxxxxx\n",
      "semiobscure\n",
      "bbdeluxe\n",
      "tobed\n",
      "audiitions\n",
      "pocked\n",
      "thesmszone\n",
      "evone\n",
      "inmind\n",
      "hellogorgeous\n",
      "nitw\n",
      "texd\n",
      "hopeu\n",
      "prayrs\n",
      "othrwise\n",
      "ujhhhhhhh\n",
      "sandiago\n",
      "parantella\n",
      "tonexs\n",
      "clubzed\n",
      "recpt\n",
      "baaaaaaaabe\n",
      "sagamu\n",
      "overemphasise\n",
      "comfey\n",
      "laready\n",
      "ffffuuuuuuu\n",
      "julianaland\n",
      "talkbut\n",
      "wannatell\n",
      "wenwecan\n",
      "smsing\n",
      "erutupalam\n",
      "thandiyachu\n",
      "khelate\n",
      "opponenter\n",
      "dhorte\n",
      "proze\n",
      "norcorp\n",
      "cfca\n",
      "otbox\n",
      "tellmiss\n",
      "utxt\n",
      "quiteamuzing\n",
      "probpop\n",
      "satthen\n",
      "psxtra\n",
      "ppmmobilesvary\n",
      "vilikkam\n",
      "sudn\n",
      "firsg\n",
      "applyed\n",
      "amnow\n",
      "tonsolitusaswell\n",
      "bedreal\n",
      "canname\n",
      "mquiz\n",
      "presnts\n",
      "jeevithathile\n",
      "irulinae\n",
      "neekunna\n",
      "prakasamanu\n",
      "sneham\n",
      "prakasam\n",
      "ennal\n",
      "vijaykanth\n",
      "anythiing\n",
      "clubmoby\n",
      "prasanth\n",
      "ettans\n",
      "sometext\n",
      "tasts\n",
      "termsapply\n",
      "shijutta\n",
      "vinobanagar\n",
      "lonlines\n",
      "gailxx\n",
      "santacalling\n",
      "shsex\n",
      "netun\n",
      "fgkslpopw\n",
      "fgkslpo\n",
      "lyricalladie\n",
      "hmmross\n",
      "gynae\n",
      "toaday\n",
      "sexychat\n",
      "frontierville\n",
      "ortxt\n",
      "experiencehttp\n",
      "etlp\n",
      "barmed\n",
      "thinkthis\n",
      "okday\n",
      "wedlunch\n",
      "outsomewhere\n",
      "adrink\n",
      "watershd\n",
      "fromwrk\n",
      "bthere\n",
      "petexxx\n",
      "olayiwola\n",
      "perumbavoor\n",
      "preponed\n",
      "skyving\n",
      "onwords\n",
      "accommodationvouchers\n",
      "mustprovide\n",
      "rajitha\n",
      "ranju\n",
      "perweeksub\n",
      "uawake\n",
      "feellikw\n",
      "justfound\n",
      "aletter\n",
      "thatmum\n",
      "gotmarried\n",
      "thnov\n",
      "ourbacks\n",
      "fuckinnice\n",
      "deviousbitch\n",
      "starer\n",
      "dysentry\n",
      "gokila\n",
      "shanil\n",
      "kotees\n",
      "customersqueries\n",
      "netvision\n",
      "haughaighgtujhyguj\n",
      "fassyole\n",
      "londn\n",
      "aslamalaikkum\n",
      "tohar\n",
      "muht\n",
      "mahfuuz\n",
      "enufcredeit\n",
      "tocall\n",
      "okors\n",
      "ibored\n",
      "goigng\n",
      "velusamy\n",
      "karnan\n",
      "geoenvironmental\n",
      "telediscount\n",
      "foned\n",
      "juswoke\n",
      "spinout\n",
      "uworld\n",
      "qbank\n",
      "someonone\n",
      "temales\n",
      "misundrstud\n",
      "mumhas\n",
      "beendropping\n",
      "theplace\n",
      "marandratha\n",
      "alaikkum\n",
      "sterm\n",
      "upcharge\n",
      "spacebucks\n",
      "franxx\n",
      "prometazine\n",
      "pocay\n",
      "wocay\n",
      "morrowxxxx\n",
      "youdoing\n",
      "foregate\n",
      "legitimat\n",
      "efreefone\n",
      "shortbreaks\n",
      "planettalkinstant\n",
      "saibaba\n",
      "colany\n",
      "gsex\n",
      "chatlines\n",
      "servs\n",
      "childporn\n",
      "pshew\n",
      "nmde\n"
     ]
    }
   ],
   "source": [
    "word_vec = np.zeros((vocab_size,200))\n",
    "\n",
    "for index, word in tok.index_word.items():\n",
    "    glove_match = glove_dict.get(word)\n",
    "    if glove_match is not None:\n",
    "        word_vec[index] = glove_match\n",
    "    else: \n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = padded_seq\n",
    "y=df.label.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "112/112 [==============================] - 4s 21ms/step - loss: 0.1972 - accuracy: 0.9265 - val_loss: 0.0866 - val_accuracy: 0.9753\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 2s 14ms/step - loss: 0.0508 - accuracy: 0.9885 - val_loss: 0.0680 - val_accuracy: 0.9776\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 1s 13ms/step - loss: 0.0264 - accuracy: 0.9966 - val_loss: 0.0626 - val_accuracy: 0.9787\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 1s 13ms/step - loss: 0.0158 - accuracy: 0.9978 - val_loss: 0.0610 - val_accuracy: 0.9787\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 1s 12ms/step - loss: 0.0105 - accuracy: 0.9989 - val_loss: 0.0579 - val_accuracy: 0.9809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x202b8d424c0>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Embedding, Dense, Conv1D, MaxPool1D, GlobalMaxPool1D\n",
    "\n",
    "\n",
    "LAYERS = [\n",
    "    Embedding(input_dim=vocab_size, output_dim=200,input_length=maxlen, trainable=False, weights = [word_vec]),\n",
    "    Conv1D(32, 8, activation='relu'),\n",
    "    MaxPool1D(2),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "]\n",
    "\n",
    "model = tf.keras.models.Sequential(LAYERS)\n",
    "\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "LOSS = 'binary_crossentropy'#tf.keras.losses.BinaryCrossentropy\n",
    "METRICS = ['accuracy']\n",
    "\n",
    "model.compile(optimizer=OPTIMIZER, loss = LOSS, metrics=METRICS)\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n"
     ]
    }
   ],
   "source": [
    "test_sentence = [\"Thanks for your subscription to Ringtone UK your mobile will be charged 5/month Please confirm by replying YES or NO. If you reply NO you will not be charged\"]\n",
    "\n",
    "def get_encode(sent):\n",
    "    seq = tok.texts_to_sequences(test_sentence)\n",
    "    padded_seq = pad_sequences(seq, padding=\"post\", maxlen=maxlen)\n",
    "    return padded_seq\n",
    "\n",
    "if model.predict(get_encode(test_sentence))>0.5:\n",
    "    print(\"spam\")\n",
    "else:\n",
    "    print(\"Not Spam\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
