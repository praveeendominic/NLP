{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dom_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdzFVVq0v1jv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c05bf75-93c1-447f-f2b5-b46865b450b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dgsim8oP8X17",
        "outputId": "291f70c1-1513-4ab5-861a-45df5b9be8a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "\n",
            "Collections:\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "\n",
            "Collections:\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "\n",
            "Collections:\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "\n",
            "Collections:\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "\n",
            "Collections:\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> \n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "\n",
            "Collections:\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "\n",
            "Collections:\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "\n",
            "Collections:\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "\n",
            "Collections:\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "\n",
            "Collections:\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> \n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "PGHTKZTS8dIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "para=\"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
        "               the world have come and invaded us, captured our lands, conquered our minds. \n",
        "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
        "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
        "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
        "               We have not grabbed their land, their culture, \n",
        "               their history and tried to enforce our way of life on them. \n",
        "               Why? Because we respect the freedom of others.That is why my \n",
        "               first vision is that of freedom. I believe that India got its first vision of \n",
        "               this in 1857, when we started the War of Independence. It is this freedom that\n",
        "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
        "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
        "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
        "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
        "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
        "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
        "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
        "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
        "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
        "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
        "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
        "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
        "               I see four milestones in my career\"\"\""
      ],
      "metadata": {
        "id": "NTk8UvCz9MOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TOKENIZATION\n",
        "\n",
        "#SENTENCE TOKENS:\n",
        "nltk.sent_tokenize(para)\n",
        "\n",
        "#WORD TOKENIZE\n",
        "nltk.word_tokenize(para)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlzdUs-B9Tw2",
        "outputId": "2d5c8db8-197e-4a3e-d47d-92ed5f605583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'have',\n",
              " 'three',\n",
              " 'visions',\n",
              " 'for',\n",
              " 'India',\n",
              " '.',\n",
              " 'In',\n",
              " '3000',\n",
              " 'years',\n",
              " 'of',\n",
              " 'our',\n",
              " 'history',\n",
              " ',',\n",
              " 'people',\n",
              " 'from',\n",
              " 'all',\n",
              " 'over',\n",
              " 'the',\n",
              " 'world',\n",
              " 'have',\n",
              " 'come',\n",
              " 'and',\n",
              " 'invaded',\n",
              " 'us',\n",
              " ',',\n",
              " 'captured',\n",
              " 'our',\n",
              " 'lands',\n",
              " ',',\n",
              " 'conquered',\n",
              " 'our',\n",
              " 'minds',\n",
              " '.',\n",
              " 'From',\n",
              " 'Alexander',\n",
              " 'onwards',\n",
              " ',',\n",
              " 'the',\n",
              " 'Greeks',\n",
              " ',',\n",
              " 'the',\n",
              " 'Turks',\n",
              " ',',\n",
              " 'the',\n",
              " 'Moguls',\n",
              " ',',\n",
              " 'the',\n",
              " 'Portuguese',\n",
              " ',',\n",
              " 'the',\n",
              " 'British',\n",
              " ',',\n",
              " 'the',\n",
              " 'French',\n",
              " ',',\n",
              " 'the',\n",
              " 'Dutch',\n",
              " ',',\n",
              " 'all',\n",
              " 'of',\n",
              " 'them',\n",
              " 'came',\n",
              " 'and',\n",
              " 'looted',\n",
              " 'us',\n",
              " ',',\n",
              " 'took',\n",
              " 'over',\n",
              " 'what',\n",
              " 'was',\n",
              " 'ours',\n",
              " '.',\n",
              " 'Yet',\n",
              " 'we',\n",
              " 'have',\n",
              " 'not',\n",
              " 'done',\n",
              " 'this',\n",
              " 'to',\n",
              " 'any',\n",
              " 'other',\n",
              " 'nation',\n",
              " '.',\n",
              " 'We',\n",
              " 'have',\n",
              " 'not',\n",
              " 'conquered',\n",
              " 'anyone',\n",
              " '.',\n",
              " 'We',\n",
              " 'have',\n",
              " 'not',\n",
              " 'grabbed',\n",
              " 'their',\n",
              " 'land',\n",
              " ',',\n",
              " 'their',\n",
              " 'culture',\n",
              " ',',\n",
              " 'their',\n",
              " 'history',\n",
              " 'and',\n",
              " 'tried',\n",
              " 'to',\n",
              " 'enforce',\n",
              " 'our',\n",
              " 'way',\n",
              " 'of',\n",
              " 'life',\n",
              " 'on',\n",
              " 'them',\n",
              " '.',\n",
              " 'Why',\n",
              " '?',\n",
              " 'Because',\n",
              " 'we',\n",
              " 'respect',\n",
              " 'the',\n",
              " 'freedom',\n",
              " 'of',\n",
              " 'others.That',\n",
              " 'is',\n",
              " 'why',\n",
              " 'my',\n",
              " 'first',\n",
              " 'vision',\n",
              " 'is',\n",
              " 'that',\n",
              " 'of',\n",
              " 'freedom',\n",
              " '.',\n",
              " 'I',\n",
              " 'believe',\n",
              " 'that',\n",
              " 'India',\n",
              " 'got',\n",
              " 'its',\n",
              " 'first',\n",
              " 'vision',\n",
              " 'of',\n",
              " 'this',\n",
              " 'in',\n",
              " '1857',\n",
              " ',',\n",
              " 'when',\n",
              " 'we',\n",
              " 'started',\n",
              " 'the',\n",
              " 'War',\n",
              " 'of',\n",
              " 'Independence',\n",
              " '.',\n",
              " 'It',\n",
              " 'is',\n",
              " 'this',\n",
              " 'freedom',\n",
              " 'that',\n",
              " 'we',\n",
              " 'must',\n",
              " 'protect',\n",
              " 'and',\n",
              " 'nurture',\n",
              " 'and',\n",
              " 'build',\n",
              " 'on',\n",
              " '.',\n",
              " 'If',\n",
              " 'we',\n",
              " 'are',\n",
              " 'not',\n",
              " 'free',\n",
              " ',',\n",
              " 'no',\n",
              " 'one',\n",
              " 'will',\n",
              " 'respect',\n",
              " 'us',\n",
              " '.',\n",
              " 'My',\n",
              " 'second',\n",
              " 'vision',\n",
              " 'for',\n",
              " 'India',\n",
              " '’',\n",
              " 's',\n",
              " 'development',\n",
              " '.',\n",
              " 'For',\n",
              " 'fifty',\n",
              " 'years',\n",
              " 'we',\n",
              " 'have',\n",
              " 'been',\n",
              " 'a',\n",
              " 'developing',\n",
              " 'nation',\n",
              " '.',\n",
              " 'It',\n",
              " 'is',\n",
              " 'time',\n",
              " 'we',\n",
              " 'see',\n",
              " 'ourselves',\n",
              " 'as',\n",
              " 'a',\n",
              " 'developed',\n",
              " 'nation',\n",
              " '.',\n",
              " 'We',\n",
              " 'are',\n",
              " 'among',\n",
              " 'the',\n",
              " 'top',\n",
              " '5',\n",
              " 'nations',\n",
              " 'of',\n",
              " 'the',\n",
              " 'world',\n",
              " 'in',\n",
              " 'terms',\n",
              " 'of',\n",
              " 'GDP',\n",
              " '.',\n",
              " 'We',\n",
              " 'have',\n",
              " 'a',\n",
              " '10',\n",
              " 'percent',\n",
              " 'growth',\n",
              " 'rate',\n",
              " 'in',\n",
              " 'most',\n",
              " 'areas',\n",
              " '.',\n",
              " 'Our',\n",
              " 'poverty',\n",
              " 'levels',\n",
              " 'are',\n",
              " 'falling',\n",
              " '.',\n",
              " 'Our',\n",
              " 'achievements',\n",
              " 'are',\n",
              " 'being',\n",
              " 'globally',\n",
              " 'recognised',\n",
              " 'today',\n",
              " '.',\n",
              " 'Yet',\n",
              " 'we',\n",
              " 'lack',\n",
              " 'the',\n",
              " 'self-confidence',\n",
              " 'to',\n",
              " 'see',\n",
              " 'ourselves',\n",
              " 'as',\n",
              " 'a',\n",
              " 'developed',\n",
              " 'nation',\n",
              " ',',\n",
              " 'self-reliant',\n",
              " 'and',\n",
              " 'self-assured',\n",
              " '.',\n",
              " 'Isn',\n",
              " '’',\n",
              " 't',\n",
              " 'this',\n",
              " 'incorrect',\n",
              " '?',\n",
              " 'I',\n",
              " 'have',\n",
              " 'a',\n",
              " 'third',\n",
              " 'vision',\n",
              " '.',\n",
              " 'India',\n",
              " 'must',\n",
              " 'stand',\n",
              " 'up',\n",
              " 'to',\n",
              " 'the',\n",
              " 'world',\n",
              " '.',\n",
              " 'Because',\n",
              " 'I',\n",
              " 'believe',\n",
              " 'that',\n",
              " 'unless',\n",
              " 'India',\n",
              " 'stands',\n",
              " 'up',\n",
              " 'to',\n",
              " 'the',\n",
              " 'world',\n",
              " ',',\n",
              " 'no',\n",
              " 'one',\n",
              " 'will',\n",
              " 'respect',\n",
              " 'us',\n",
              " '.',\n",
              " 'Only',\n",
              " 'strength',\n",
              " 'respects',\n",
              " 'strength',\n",
              " '.',\n",
              " 'We',\n",
              " 'must',\n",
              " 'be',\n",
              " 'strong',\n",
              " 'not',\n",
              " 'only',\n",
              " 'as',\n",
              " 'a',\n",
              " 'military',\n",
              " 'power',\n",
              " 'but',\n",
              " 'also',\n",
              " 'as',\n",
              " 'an',\n",
              " 'economic',\n",
              " 'power',\n",
              " '.',\n",
              " 'Both',\n",
              " 'must',\n",
              " 'go',\n",
              " 'hand-in-hand',\n",
              " '.',\n",
              " 'My',\n",
              " 'good',\n",
              " 'fortune',\n",
              " 'was',\n",
              " 'to',\n",
              " 'have',\n",
              " 'worked',\n",
              " 'with',\n",
              " 'three',\n",
              " 'great',\n",
              " 'minds',\n",
              " '.',\n",
              " 'Dr.',\n",
              " 'Vikram',\n",
              " 'Sarabhai',\n",
              " 'of',\n",
              " 'the',\n",
              " 'Dept',\n",
              " '.',\n",
              " 'of',\n",
              " 'space',\n",
              " ',',\n",
              " 'Professor',\n",
              " 'Satish',\n",
              " 'Dhawan',\n",
              " ',',\n",
              " 'who',\n",
              " 'succeeded',\n",
              " 'him',\n",
              " 'and',\n",
              " 'Dr.',\n",
              " 'Brahm',\n",
              " 'Prakash',\n",
              " ',',\n",
              " 'father',\n",
              " 'of',\n",
              " 'nuclear',\n",
              " 'material',\n",
              " '.',\n",
              " 'I',\n",
              " 'was',\n",
              " 'lucky',\n",
              " 'to',\n",
              " 'have',\n",
              " 'worked',\n",
              " 'with',\n",
              " 'all',\n",
              " 'three',\n",
              " 'of',\n",
              " 'them',\n",
              " 'closely',\n",
              " 'and',\n",
              " 'consider',\n",
              " 'this',\n",
              " 'the',\n",
              " 'great',\n",
              " 'opportunity',\n",
              " 'of',\n",
              " 'my',\n",
              " 'life',\n",
              " '.',\n",
              " 'I',\n",
              " 'see',\n",
              " 'four',\n",
              " 'milestones',\n",
              " 'in',\n",
              " 'my',\n",
              " 'career']"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEMMING\n",
        "sw=stopwords.words('english')\n",
        "stemmer=PorterStemmer()\n",
        "\n",
        "\n",
        "sentences=nltk.sent_tokenize(para)\n"
      ],
      "metadata": {
        "id": "l75wvGAX9fp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEMMING\n",
        "for i in range(len(sentences)):\n",
        "  words=nltk.word_tokenize(sentences[i])\n",
        "  words=[stemmer.stem(x) for x in words if x not in sw]\n",
        "  sentences[i]=' '.join(words)"
      ],
      "metadata": {
        "id": "hWsH6QWW95gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LEMMATIZATION\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "sw=stopwords.words('english')\n",
        "\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "\n",
        "sentences=nltk.sent_tokenize(para)\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  words=nltk.word_tokenize(sentences[i])\n",
        "  words=[lemmatizer.lemmatize(x) for x in words if x not in sw]\n",
        "  sentences[i]=' '.join(words)\n"
      ],
      "metadata": {
        "id": "9Mnwq7Fn-kkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9QkyTCw_O3j",
        "outputId": "20781244-b81f-47f6-ef68-bf4006bb36f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I three vision India .',\n",
              " 'In 3000 year history , people world come invaded u , captured land , conquered mind .',\n",
              " 'From Alexander onwards , Greeks , Turks , Moguls , Portuguese , British , French , Dutch , came looted u , took .',\n",
              " 'Yet done nation .',\n",
              " 'We conquered anyone .',\n",
              " 'We grabbed land , culture , history tried enforce way life .',\n",
              " 'Why ?',\n",
              " 'Because respect freedom others.That first vision freedom .',\n",
              " 'I believe India got first vision 1857 , started War Independence .',\n",
              " 'It freedom must protect nurture build .',\n",
              " 'If free , one respect u .',\n",
              " 'My second vision India ’ development .',\n",
              " 'For fifty year developing nation .',\n",
              " 'It time see developed nation .',\n",
              " 'We among top 5 nation world term GDP .',\n",
              " 'We 10 percent growth rate area .',\n",
              " 'Our poverty level falling .',\n",
              " 'Our achievement globally recognised today .',\n",
              " 'Yet lack self-confidence see developed nation , self-reliant self-assured .',\n",
              " 'Isn ’ incorrect ?',\n",
              " 'I third vision .',\n",
              " 'India must stand world .',\n",
              " 'Because I believe unless India stand world , one respect u .',\n",
              " 'Only strength respect strength .',\n",
              " 'We must strong military power also economic power .',\n",
              " 'Both must go hand-in-hand .',\n",
              " 'My good fortune worked three great mind .',\n",
              " 'Dr. Vikram Sarabhai Dept .',\n",
              " 'space , Professor Satish Dhawan , succeeded Dr. Brahm Prakash , father nuclear material .',\n",
              " 'I lucky worked three closely consider great opportunity life .',\n",
              " 'I see four milestone career']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BOW\n",
        "\n",
        "# 1. clean text- stopwrords removal, number removal, puntuations removal, stemming/lemmatization\n",
        "# 2. Vectorization\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import re\n",
        "\n",
        "sentences= nltk.sent_tokenize(para)\n",
        "corpus=[]\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  sentences[i]=re.sub('[^a-zA-Z]',' ',sentences[i])\n",
        "  sentences[i]=sentences[i].lower()\n",
        "  words=nltk.word_tokenize(sentences[i])\n",
        "  \n",
        "  words=[lemmatizer.lemmatize(x) for x in words if x not in stopwords.words('english')]\n",
        "  sent=' '.join(words)\n",
        "  corpus.append(sent)\n",
        "\n",
        "# print(corpus)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv=CountVectorizer()\n",
        "bow=cv.fit_transform(corpus).toarray()\n",
        "bow\n",
        "bow.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wv-1Rd5X_wXG",
        "outputId": "08bb6703-8733-4bf5-9551-4c6baafb3fbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31, 114)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TFIDF\n",
        "\n",
        "# 1. clean text- eliminate stopwords, punctuations, numerics, stemming and lemmatizer, convert lowercase\n",
        "# 2. build vector TFIDF\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "sentences=nltk.sent_tokenize(para)\n",
        "\n",
        "corpus=[]\n",
        "\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  sentences[i]=re.sub('[^a-zA-Z]',' ',sentences[i])\n",
        "  sentences[i]=sentences[i].lower()\n",
        "  words=nltk.word_tokenize(sentences[i])\n",
        "\n",
        "  words=[lemmatizer.lemmatize(x)for x in words if x not in stopwords.words('english')]\n",
        "   \n",
        "  sentences[i]=' '.join(words)\n",
        "  corpus.append(sentences[i])\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf=TfidfVectorizer()\n",
        "vec=tfidf.fit_transform(corpus).toarray()\n",
        "vec.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TDEO7hX_6JP",
        "outputId": "1da7e0a6-3f90-451f-b49e-ba5f1d32eb88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31, 114)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WORD2VEC\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "sentences=nltk.sent_tokenize(para)\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  sentences[i]=re.sub('[^a-zA-Z]',' ',sentences[i]).lower()\n",
        "  sentences[i]=re.sub(r'\\s+',' ',sentences[i])\n",
        "  sentences[i]=re.sub(r'\\d',' ',sentences[i])\n",
        "  \n",
        "  words=nltk.word_tokenize(sentences[i])\n",
        "  words=[WordNetLemmatizer().lemmatize(x) for x in words if x not in stopwords.words('english')]\n",
        "  sentences[i]=words\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "wv=Word2Vec(sentences,min_count=1)\n",
        "wv.wv.vocab\n",
        "wv.wv.most_similar('india')\n",
        "# wv.wv['india']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW0XRrvBNoWm",
        "outputId": "c13fc22f-9931-4671-dfa1-49191c44f12f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('dept', 0.24164697527885437),\n",
              " ('history', 0.23020336031913757),\n",
              " ('hand', 0.19205087423324585),\n",
              " ('grabbed', 0.1794241964817047),\n",
              " ('go', 0.15988847613334656),\n",
              " ('mogul', 0.15881657600402832),\n",
              " ('took', 0.15836533904075623),\n",
              " ('captured', 0.15346553921699524),\n",
              " ('level', 0.152181476354599),\n",
              " ('falling', 0.15132567286491394)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# REGULAR EXPRESSION\n",
        "import re\n",
        "txt=\"\"\"Chris is my dad. He is reborn on 2017-07-09. He is the grandson of Dharma.\"\"\"\n",
        "\n",
        "\n",
        "pattern='Chris ([a-z]+) my' #get word between Chris and my\n",
        "re.findall(pattern,txt)\n",
        "\n",
        "pattern=' [a-z]{2} ' #get all 2 letter words in the sentence\n",
        "re.findall(pattern,txt)\n",
        "\n",
        "pattern='[a-zA-Z]{4,}' #get all 4 or more letter words\n",
        "re.findall(pattern,txt)\n",
        "\n",
        "pattern='\\d{4}-\\d{2}-\\d{2}' #get DOB\n",
        "re.findall(pattern,txt)\n",
        "\n",
        "# pattern='[a-zA-Z]*[^\\s\\d\\-\\.]' #retrieve all words except whitespaces and punctuations\n",
        "# re.findall(pattern,txt)\n",
        "\n",
        "pattern='[a-zA-Z]*\\.$|\\d{4}-\\d{2}-\\d{2}\\.$'#Get the last word of the text\n",
        "re.findall(pattern,txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrtWWvHVP_CO",
        "outputId": "6785a7cd-069e-4838-ddfe-e99c64990ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dharma.']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Extract all twitter handles from following text. Twitter handle is the text that appears after https://twitter.com/ and is a single word. Also it contains only alpha numeric characters i.e. A-Z a-z , o to 9 and underscore _\n",
        "\n",
        "text = '''\n",
        "Follow our leader Elon musk on twitter here: https://twitter.com/elonmusk, more information on Tesla's products can be found at https://www.tesla.com/. Also here are leading influencers \n",
        "for tesla related news,\n",
        "https://twitter.com/teslarati\n",
        "https://twitter.com/dummy_tesla\n",
        "https://twitter.com/dummy_2_tesla\n",
        "'''\n",
        "pattern = 'twitter.com\\/([a-zA-Z\\_\\d{1,}]*)' # todo: type your regex here\n",
        "\n",
        "re.findall(pattern, text)\n"
      ],
      "metadata": {
        "id": "BMPAQZGIPyS5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac4a2fc0-249c-44eb-ccd1-8e75a4f9ff5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['elonmusk,', 'teslarati', 'dummy_tesla', 'dummy_2_tesla']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Extract Concentration Risk Types. It will be a text that appears after \"Concentration Risk:\", In below example, your regex should extract these two strings\n",
        "\n",
        "text = '''\n",
        "Concentration of Risk: Credit Risk\n",
        "Financial instruments that potentially subject us to a concentration of credit risk consist of cash, cash equivalents, marketable securities,\n",
        "restricted cash, accounts receivable, convertible note hedges, and interest rate swaps. Our cash balances are primarily invested in money market funds\n",
        "or on deposit at high credit quality financial institutions in the U.S. These deposits are typically in excess of insured limits. As of September 30, 2021\n",
        "and December 31, 2020, no entity represented 10% or more of our total accounts receivable balance. The risk of concentration for our convertible note\n",
        "hedges and interest rate swaps is mitigated by transacting with several highly-rated multinational banks.\n",
        "Concentration of Risk: Supply Risk\n",
        "We are dependent on our suppliers, including single source suppliers, and the inability of these suppliers to deliver necessary components of our\n",
        "products in a timely manner at prices, quality levels and volumes acceptable to us, or our inability to efficiently manage these components from these\n",
        "suppliers, could have a material adverse effect on our business, prospects, financial condition and operating results.\n",
        "'''\n",
        "pattern = ': ([^\\n]*)' # todo: type your regex here\n",
        "\n",
        "re.findall(pattern, text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDm7xrbI7cs2",
        "outputId": "1fa72706-866a-43bc-8933-c35ba2cd2fec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Credit Risk', 'Supply Risk']"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Companies in europe reports their financial numbers of semi annual basis and you can have a document like this. To exatract quarterly and semin annual period you can use a regex as shown below\n",
        "text = '''\n",
        "Tesla's gross cost of operating lease vehicles in FY2021 Q1 was $4.85 billion.\n",
        "BMW's gross cost of operating vehicles in FY2021 S1 was $8 billion.\n",
        "'''\n",
        "\n",
        "pattern = 'FY(\\d{4} (?:Q[1-4]|S[1-2]))'\n",
        "matches = re.findall(pattern, text)\n",
        "matches"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5K0UInJTIWS",
        "outputId": "deaa62e5-adac-4574-8fdc-356b2c2f12ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2021 Q1', '2021 S1']"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute amount for each period provided\n",
        "pattern='(FY\\d{4}\\s*[A-Z][1-4])[^$]*(\\$[^a-zA-Z]*[^\\s]*)'\n",
        "matches = re.findall(pattern, text)\n",
        "print(matches)\n",
        "\n",
        "# re.search(pattern,text).group() #Retrieves 1st hit of the\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snLm8GYSUoUq",
        "outputId": "207aaab5-f1c7-44f8-c69c-435128823162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('FY2021 Q1', '$4.85 billion.'), ('FY2021 S1', '$8 billion.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sachin=\"\"\"Full name\tSachin Ramesh Tendulkar\n",
        "Born\t24 April 1973 (age 49)\n",
        "Bombay, Maharashtra, India\n",
        "Nickname\tLittle Master, Master Blaster[1][2]\n",
        "Height\t165 cm (5 ft 5 in)\n",
        "Batting\tRight-handed\n",
        "Bowling\tRight-arm leg break\n",
        "Right-arm off break\n",
        "Role\tTop-order batsman\"\"\"\n",
        "\n",
        "\n",
        "rohit=\"\"\"Full name\tRohit Gurunath Sharma\n",
        "Born\t30 April 1987 (age 35)\n",
        "Nagpur, Maharashtra, India\n",
        "Nickname\tHitman[1]\n",
        "Height\t1.75 m (5 ft 9 in)\n",
        "Batting\tRight-handed\n",
        "Bowling\tRight-arm off break\n",
        "Role\tOpening batsman\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "def get_name_age_place(text):\n",
        "  fname_pattern='Full name(.*)'\n",
        "  matches = re.findall(fname_pattern, text)\n",
        "  name=matches[0].strip()\n",
        "\n",
        "  db_pattern='Born([0-9a-zA-Z\\s-]*)'\n",
        "  matches = re.findall(db_pattern, text)\n",
        "  dob=matches[0].strip()\n",
        "\n",
        "  place_pattern='age.*\\n(.*)'\n",
        "  matches = re.findall(place_pattern, text)\n",
        "  place=matches[0].strip()\n",
        "\n",
        "  return {'Name':name,\n",
        "          'DOB':dob,\n",
        "          'place':place\n",
        "          }\n",
        "\n"
      ],
      "metadata": {
        "id": "9VyK7XBlWs6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_name_age_place(sachin)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0TowsSumYYJ",
        "outputId": "9fb7bb24-3251-43f6-9d39-bb1c89482df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DOB': '24 April 1973',\n",
              " 'Name': 'Sachin Ramesh Tendulkar',\n",
              " 'place': 'Bombay, Maharashtra, India'}"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_name_age_place(rohit)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfY629gGsHil",
        "outputId": "b5105bf6-5c92-4ff8-d0c2-7f38fd9fd5f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DOB': '30 April 1987',\n",
              " 'Name': 'Rohit Gurunath Sharma',\n",
              " 'place': 'Nagpur, Maharashtra, India'}"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WLNSEgGvsKgQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}